# -*- coding: utf-8 -*-
"""Copy of Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BnJQwcUWA23gXPZeNagg5WS3MRwCAEui
"""

#Load the libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud,STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
#import spacy
import re,string,unicodedata
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem import LancasterStemmer,WordNetLemmatizer
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from textblob import TextBlob
from textblob import Word
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

import os
import warnings
warnings.filterwarnings('ignore')
!pip install afinn
!pip install vaderSentiment
#installing in jupyter

#importing necessary libraries
from afinn import Afinn
import nltk
nltk.download('stopwords')
import csv
imdb_data=pd.read_csv('./IMDB_Dataset.csv',error_bad_lines = False, engine = "python")
print(imdb_data.shape)
imdb_data.head(10)

#split the dataset
#train dataset
train_reviews=imdb_data.review[:50000]
train_sentiments=imdb_data.sentiment[:50000]
# #test dataset
# test_reviews=imdb_data.review[40000:]
# test_sentiments=imdb_data.sentiment[40000:]
print(train_reviews.shape,train_sentiments.shape)
# print(test_reviews.shape,test_sentiments.shape)

#Tokenization of text
tokenizer=ToktokTokenizer()
#Setting English stopwords
stopword_list=nltk.corpus.stopwords.words('english')


#Removing the html strips
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()


#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)


#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

#Apply function on review column
imdb_data['review']=imdb_data['review'].apply(denoise_text)


#Define function for removing special characters
def remove_special_characters(text, remove_digits=True):
    pattern=r'[^a-zA-z0-9\s]'
    text=re.sub(pattern,'',text)
    return text


#Apply function on review column
imdb_data['review']=imdb_data['review'].apply(remove_special_characters)


#Stemming the text
def simple_stemmer(text):
    ps=nltk.porter.PorterStemmer()
    text= ' '.join([ps.stem(word) for word in text.split()])
    return text


#Apply function on review column
imdb_data['review']=imdb_data['review'].apply(simple_stemmer)


#set stopwords to english
stop=set(stopwords.words('english'))
print(stop)

#removing the stopwords
def remove_stopwords(text, is_lower_case=False):
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        filtered_tokens = [token for token in tokens if token not in stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text
#Apply function on review column
imdb_data['review']=imdb_data['review'].apply(remove_stopwords)
# #normalized train reviews
# norm_train_reviews=imdb_data.review[:50000]
# norm_train_reviews[0]
# #convert dataframe to string
# norm_train_string=norm_train_reviews.to_string()
# #Spelling correction using Textblob
# norm_train_spelling=TextBlob(norm_train_string)
# norm_train_spelling.correct()
# #Tokenization using Textblob
# norm_train_words=norm_train_spelling.words
# norm_train_words

# #Normalized test reviews
# norm_test_reviews=imdb_data.review[40000:]
# norm_test_reviews[45005]
# #convert dataframe to string
# norm_test_string=norm_test_reviews.to_string()
# #spelling correction using Textblob
# norm_test_spelling=TextBlob(norm_test_string)
# print(norm_test_spelling.correct())
# #Tokenization using Textblob
# norm_test_words=norm_test_spelling.words
# norm_test_words

#instantiate afinn
afn = Afinn()

# compute scores (polarity) and labels
scores = [afn.score(article) for article in imdb_data['review']]
sentiment = ['positive' if score > 0 else 'negative' if score <= 0 else 'neutral' for score in scores]

# dataframe creation
af = pd.DataFrame()
af['topic'] = imdb_data['review']
af['scores'] = scores
af['computed sentiments'] = sentiment
af['original sentiments'] = imdb_data['sentiment']
af.head(10)

count=0
for i in range (0,50000) :
  # print(i)
  if sentiment[i]==imdb_data['sentiment'][i] :
    count=count+1

score=(count/50000.0)*100;
print("\n\n\n",score)

afn = Afinn()

imdb_data=pd.read_csv('./IMDB_Dataset.csv',error_bad_lines = False, engine = "python")
# compute scores (polarity) and labels

scores = [afn.score(article) for article in imdb_data['review']]
sentiment = ['positive' if score > 0 else 'negative' if score <= 0 else 'neutral' for score in scores]

# dataframe creation
af = pd.DataFrame()
af['topic'] = imdb_data['review']
af['scores'] = scores
af['computed sentiments'] = sentiment
af['original sentiments'] = imdb_data['sentiment']
af.head(10)

test_acc = 0
br = 0
for i in range(0,100):
  k = i/100*2 - 1;
  sentiment = ['positive' if score > k else 'negative' if score <= k else 'neutral' for score in scores]
  count=0
  score = accuracy_score(sentiment , imdb_data['sentiment'])
  if score>test_acc:
    br = i
    test_acc = score
  # print(score)
print("\n\n\n",score)

print(br)

af.head(10)

count=0
for i in range (0,50000) :
  # print(i)
  if sentiment[i]==imdb_data['sentiment'][i]:
    count=count+1

score=(count/50000.0)*100;
print("\n\n\n",score)

af['adjusted score'] = 1
for i in range (0,50000):
    af['adjusted score'][i] = af['scores'][i]/len(af['topic'][i].split())
    af['adjusted score'][i] = (af['adjusted score'][i]+5)/10
af

test_acc = 0
br = 0
af['adjusted sentiment'] = 'positive'
for i in range(200,400):
    af['adjusted sentiment'] = 'positive'
    af.loc[af['adjusted score']<i/500,'adjusted sentiment'] = 'negative'
    score = accuracy_score(af['adjusted sentiment'],af['original sentiments'])
    print(score)
    if score > test_acc:
        br = i
        test_acc = score
print("\n\n\n",test_acc,br)
af['adjusted sentiment'] = 'positive'
af.loc[af['adjusted score']<br/500,'adjusted sentiment'] = 'negative'

from textblob import TextBlob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
original_data = pd.read_csv('./IMDB_Dataset.csv')
df = original_data

# df = df.drop(columns='sentiment')

df['polarity'] = df.apply(lambda x: TextBlob(x['review']).sentiment.polarity, axis=1)
# df['subjectivity'] = df.apply(lambda x: TextBlob(x['review']).sentiment.subjectivity, axis=1 )
df.head(10)

from sklearn.metrics import mean_absolute_error , mean_squared_error ,accuracy_score
y = 0
h = 0
acc_matrix = pd.DataFrame(columns=['polarity threshold', 'subjectivity threshold', 'accuracy score'])
for i in range (0,200):
    k = i/100 - 1
    df['answer'] = 'positive'
    df.loc[df['polarity']<k,'answer'] = 'negative'
    x = accuracy_score(df['sentiment'] , df['answer'])
    new_row = {'polarity threshold':i, 'subjectivity threshold': j, 'accuracy score': x}
    acc_matrix.loc[len(acc_matrix)] = new_row
    if h<x:
        h = x
        y = i
print(h,y)
df['answer'] = 'positive'
df.loc[df['polarity']<y/100-1,'answer'] = 'negative'

df['adjusted score'] = 1
for i in range (0,50000):
    df['adjusted score'][i] = (df['polarity'][i]+1)/2
test_acc = 0
br = 0
df['adjusted sentiment'] = 'positive'
for i in range(0,400):
    df['adjusted sentiment'] = 'positive'
    df.loc[df['adjusted score']<i/400,'adjusted sentiment'] = 'negative'
    score = accuracy_score(df['adjusted sentiment'],df['sentiment'])
#     print(score)
    if score > test_acc:
        br = i
        test_acc = score
print("\n\n\n",test_acc)
df['adjusted sentiment'] = 'positive'
df.loc[df['adjusted score']<br/400,'adjusted sentiment'] = 'negative'

df['answer'] = 1

original_data['answer'] = 1
original_data.loc[original_data['sentiment'] == "negative", 'answer'] = -1

from sklearn.metrics import mean_absolute_error , mean_squared_error ,accuracy_score
y = [0,0]
h = 0
acc_matrix = pd.DataFrame(columns=['polarity threshold', 'subjectivity threshold', 'accuracy score'])
for i in range (40,60):
    k = i/100*2 - 1
    df['answer'] = 1
    df.loc[df['polarity']<k,'answer'] = -1
    for j in range (0,100):
        l = j/100
        df.loc[df['subjectivity']<l,'answer'] = -1
        x = accuracy_score(original_data['answer'] , df['answer'])
        acc_matrix = acc_matrix.append({'polarity threshold': i, 'subjectivity threshold': j, 'accuracy score': x}, ignore_index=True)
        if x > h :
            h = x
            y = [i,j]
print(y)

print(h)

acc_matrix.head(15)

df['answer'] = 1
df.loc[df['polarity']<y[0]/100*2 - 1,'answer'] = -1
df.loc[df['subjectivity']<=y[1]/100,'answer'] = 1
mean_absolute_error(original_data['answer'] , df['answer'])

print('RMSE :' ,mean_squared_error(original_data['answer'] , df['answer']))

accuracy_score(original_data['answer'] , df['answer'])

af['scores'].max() , af['scores'].min()

for i in range(0,50000):
  if af['scores'][i] > 0:
    af['scores'][i] = af['scores'][i]/184
  else:
    af['scores'][i] = af['scores'][i]/127

af['value'] = 1
af.loc[af['computed sentiments']=='negative','value']=-1
af['o value'] = 1
af.loc[af['original sentiments']=='negative','o value']=-1

new_data = pd.DataFrame()
new_data['review'] = original_data['review']
new_data['afin score'] = af['scores']
new_data['textbl score'] = df['polarity']
new_data['orig sentiments'] = af['original sentiments']

new_data['orig'] = 1
new_data.loc[new_data['orig sentiments'] =='negative','orig'] = -1
new_data['afin'] = af['value']
new_data['textbl'] = df['answer']
new_data['mix score']  = new_data['afin score']+new_data['textbl score']
print(accuracy_score(new_data['afin'],new_data['orig']), accuracy_score(new_data['textbl'],new_data['orig']))

h = 0
y=[0,0,0]
for i in range (35,51):
 for j in range (25,51):
    new_data['mix score']  = new_data['afin score']*j/50 + new_data['textbl score']*i/50
    for k in range (20,30):
      new_data['pred'] = 1
      new_data.loc[new_data['mix score']<k/50*2-1,'pred'] = -1
      p = accuracy_score(new_data['orig'], new_data['pred'])
      if p>h:
        h = p
        y= [i,j,k]
print(h,y)



accuracy_score(af['value'],af['o value'])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
from tqdm import tqdm


cd = pd.read_csv("./IMDB_Dataset.csv")
res = {}

for i, row in tqdm(cd.iterrows(), total=len(cd)):
    txt = row['review']
    txt.lower()
    res[i] = sia.polarity_scores(txt)

cd['score'] = res
cd['compound'] = cd['score'].apply(lambda score_dict: score_dict['compound'])
val={}
i=0
for row in cd.itertuples():
    if row[4]>=0:
        val[i]='positive'
    else:
        val[i]='negative'
    i=i+1

cd['comp_score']=val

print(accuracy_score(cd['sentiment'],cd['comp_score']))
cd

s = 0
itt = 0
for j in range (0,500):
    cd['comp_score']='positive'
    cd.loc[cd['compound']<j/250-1,'comp_score'] = 'negative'
    p = accuracy_score(cd['sentiment'],cd['comp_score'])
    if s<p:
        itt = j
        s = p
print(s,itt)

cd['adjusted score'] = 1
for i in range (0,50000):
    cd['adjusted score'][i] = (cd['compound'][i]+1)/2
test_acc = 0
br = 0
cd['adjusted sentiment'] = 'positive'
for i in range(0,500):
    cd['adjusted sentiment'] = 'positive'
    cd.loc[cd['adjusted score']<i/500,'adjusted sentiment'] = 'negative'
    score = accuracy_score(cd['adjusted sentiment'],cd['sentiment'])
    print(score)
    if score > test_acc:
        br = i
        test_acc = score
print("\n\n\n",test_acc,br)
cd['adjusted sentiment'] = 'positive'
cd.loc[cd['adjusted score']<br/400,'adjusted sentiment'] = 'negative'

h=0;
for i in range(0,100):
  for row in cd.itertuples():
      if row[4]>=i/100*2-1:
          val[i]='positive'
      else:
          val[i]='negative'
      i=i+1
  cd['comp_score']=val
  px = accuracy_score(cd['sentiment'],cd['comp_score'])
  print(px)
  if px>h:
    h = px
h

af['value'] = 1
af.loc[af['adjusted sentiment']=='negative','value']=-1
af['o value'] = 1
af.loc[af['original sentiments']=='negative','o value']=-1

new_data = pd.DataFrame()
new_data['review'] = imdb_data['review']
new_data['afin score'] = af['adjusted score']
new_data['textbl score'] = df['adjusted score']
new_data['vadar score'] = cd['adjusted score']
new_data['orig sentiments'] = af['original sentiments']
new_data['orig'] = 1
new_data.loc[new_data['orig sentiments'] =='negative','orig'] = -1

new_data['mix score']  = new_data['afin score']+new_data['textbl score'] + new_data['vadar score']

new_data

max_accur = 0
y=[0,0,0]
matri = pd.DataFrame(columns=['textbl w', 'afinn w', 'vadar w','accuracy'])

for i in range (55,65):
    for j in range (0,40):
        if i+j >100:
            break
        for l in range (0,40):
            if i+j+l > 100:
                break
            new_data['mix score']  = new_data['afin score']*j/100 + new_data['textbl score']*i/100 + new_data['vadar score']*l/100
            new_data['pred'] = 1
            new_data.loc[new_data['mix score']<0.5,'pred'] = -1
            accur = accuracy_score(new_data['orig'], new_data['pred'])
            new_row = {'textbl w':i, 'afinn w': j,'vadar w':l ,'accuracy': accur}
            matri.loc[len(matri)] = new_row
            print(i,j,l,accur)
            if accur>max_accur:
                max_accur = accur
                y= [i,j,l]
print(max_accur,y)

new_data['orig'] = 1
new_data.loc[new_data['orig sentiments'] =='negative','orig'] = -1
new_data['afin'] = af['value']
new_data['textbl'] = df['answer']
new_data['vadar score'] = cd['compound']
new_data['mix score']  = new_data['afin score']+new_data['textbl score'] + new_data['vadar score']
gh = pd.DataFrame()
gh['orig'] = new_data['orig']
gh['tested'] = 1
gh.loc[new_data['mix score']<0,'tested'] = -1
h=0
l=0;
for i in range (0,1000):
  gh.loc[new_data['mix score']<i/1000,'tested'] = -1
  if accuracy_score(gh['tested'],new_data['orig'])>h:
    h = accuracy_score(gh['tested'],new_data['orig'])
    l = i
print(h,i)

max_accur = 0
y=[0,0,0,0]
for i in range (44,51):
 for j in range (25,40):
  for l in range (0,50):
    first=0
    second=0
    third=0
    fourth=0
    fifth=0
    new_data['mix score']  = new_data['afin score']*j/50 + new_data['textbl score']*i/50 + new_data['vadar score']*l/50
    for k in range (20,40):
      new_data['pred'] = 1
      new_data.loc[new_data['mix score']<k/50*2-1,'pred'] = -1
      accur = accuracy_score(new_data['orig'], new_data['pred'])
      if k==20:
        first = accur
      elif k==21:
        second = accur
      elif k==22:
        third = accur
      elif k==23:
        fourth = accur
      elif k==24:
        fifth = accur
      else:
        first=second
        second=third
        third=fourth
        fourth=fifth
        fifth = accur
        if first>=second and second>=third and third>=fourth and fourth>=fifth:
          break
      # print(i,j,l,k)
      if accur>max_accur:
        max_accur = accur
        y= [i,j,l,k]
print(max_accur,y)

new_data['new pred'] = 1
for i in range (0,50000):
  if abs(new_data['afin score'][i]) > abs(new_data['vadar score'][i]):
    if abs(new_data['afin score'][i]) > abs(new_data['textbl score'][i]):
      new_data['new pred'][i] = new_data['afin score'][i]
  elif abs(new_data['afin score'][i]) < abs(new_data['vadar score'][i]):
    if abs(new_data['textbl score'][i]) < abs(new_data['vadar score'][i]):
      new_data['new pred'][i] = new_data['vadar score'][i]
  elif abs(new_data['textbl score'][i]) > abs(new_data['afin score'][i]):
    if abs(new_data['textbl score'][i]) > abs(new_data['vadar score'][i]):
      new_data['new pred'][i] = new_data['textbl score'][i]
new_data['new pred d'] = 1
new_data.loc[new_data['new pred']<0,'new pred d']= -1
accuracy_score(new_data['new pred d'],new_data['orig'])
# new_data

h = 0
y=[0,0,0,0]
for i in range (40,51):
 for j in range (5,10):
  for l in range (0,50):
    a=0
    b=0
    c=0
    d=0
    e=0
    new_data['mix score']  = new_data['afin score']*j/50 + new_data['textbl score']*i/50 + new_data['vadar score']*l/50
    for k in range (25,50):
      new_data['pred'] = 1
      new_data.loc[new_data['mix score']<k/50*2-1,'pred'] = -1
      p = accuracy_score(new_data['orig'], new_data['pred'])
      if k==20:
        a = p
      elif k==21:
        b = p
      elif k==22:
        c = p
      elif k==23:
        d = p
      elif k==24:
        e = p
      else:
        a=b
        b=c
        c=d
        d=e
        e = p
        if a>=b and b>=c and c>=d and d>=e:
          break
      # print(i,j,l,k)
      if p>h:
        h = p
        y= [i,j,l,k]
print(h,y)

print('Accuracy : ', h*100)
print('Weights ')
print('TEXTBLOB : ', y[0])
print('AFINN : ', y[1])
print('VADAR : ', y[2])
print('Threshols : ',y[3]/50*2-1)

import pandas as pd
from keras.models import Sequential
from keras.layers import Dense


# Extract the input data from the DataFrame
X = new_data[['afin score', 'textbl score', 'vadar score']]
# X = X[:25000]
# Extract the target data from the DataFrame if available
# Replace 'target' with the actual target column name
y = new_data['orig']
# y = y+1
# y = y[:25000]
# Create a sequential model
model = Sequential()

# Add the input layer with 3 neurons
model.add(Dense(units=10, input_dim=3, activation='relu'))  # Input to Hidden Layer 1

# Add the first hidden laye with 10 neurons
model.add(Dense(units=25, activation='relu'))  # Hidden Layer 1 to Hidden Layer 2
model.add(Dense(units=25, activation='relu'))  # Hidden Layer 1 to Hidden Layer 2
model.add(Dense(units=10, activation='relu'))  # Hidden Layer 1 to Hidden Layer 2
model.add(Dense(units=10, activation='relu'))  # Hidden Layer 1 to Hidden Layer 2
# model.add(Dense(units=5, activation='relu'))  # Hidden Layer 1 to Hidden Layer 2
# model.add(Dense(units=3, activation='relu'))  # Hidden Layer 1 to Hidden Layer 2

# Add the second hidden layer with 10 neurons
model.add(Dense(units=1, activation='linear'))  # Hidden Layer 2 to Output Layer

# Compile the model with an appropriate loss function and optimizer
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(X, y, epochs=150)

# Print model summary
model.summary()

# Print final loss from training
print("Final Loss:", history.history['loss'][-1])

predicts = model.predict(X)
print(predicts)

print(predicts.min(),predicts.max())

X['predi'] = predicts
X['value'] = 1
param = 0
r=0
for i in range(0,1600):
  X.loc[X['predi']<i/400-2,'value'] = -1
  p = accuracy_score(X['value'], y)
  if p>param:
    param = p
    r=i
print(param,r)

